start training
all done
We are using the state dict from
 /scratch/qz1086/CV-FinalProject/CVFramework/checkpoints/UNet_26.pth
this is the trial for L1 unstructred pruning,         prune encoder in [0.1, 0.3, 0.5, 0.7, 0.9], 	        decoder in [0.1, 0.3, 0.5, 0.7, 0.9]
the matrix notation: first for decoder 0.1, second for decoder 0.15, etc. 	        raw for encoder, colum for layers
decoder pruning ratio = 0.10
encoder pruning ratio =: 0.10
2.4954423705736795
2.5145209232966104
2.513960884677039
2.5139591693878174
2.4962038265334234
2.5139608648088245
encoder pruning ratio =: 0.30
2.499885731273227
2.9334422879748874
2.9413183795081244
2.9413145316971674
2.4962035020192466
2.9413183795081244
encoder pruning ratio =: 0.50
2.5030539168251886
3.7731685638427734
3.781007250150045
3.780999196900262
2.496201846334669
3.7810071441862316
encoder pruning ratio =: 0.70
2.506002359920078
4.981254630618626
4.97726039091746
4.977246708340115
2.4962034159236484
4.977260337935554
encoder pruning ratio =: 0.90
2.508258660634359
6.9974039395650225
6.982987682024638
6.982966581980388
2.4961990581618414
6.982987509833442
decoder pruning ratio = 0.30
encoder pruning ratio =: 0.10
2.498198495970832
2.5179020166397095
2.5173441966374717
2.5173426469167075
2.49896240234375
2.5173442165056863
encoder pruning ratio =: 0.30
2.5027244554625616
2.9366961783832974
2.9445622033543057
2.944560216532813
2.498963269922468
2.944562223222521
encoder pruning ratio =: 0.50
2.5059602393044367
3.7724477185143366
3.78031333287557
3.7803083260854087
2.4989620049794516
3.7803130679660373
encoder pruning ratio =: 0.70
2.5090116262435913
4.97455808851454
4.9707101583480835
4.970702489217122
2.4989643229378595
4.970709840456645
encoder pruning ratio =: 0.90
2.51129945119222
6.992280112372504
6.978066325187683
6.978053304884169
2.498960104253557
6.978065941068861
decoder pruning ratio = 0.50
encoder pruning ratio =: 0.10
2.4992443124453225
2.5188854270511203
2.518332925107744
2.518330454826355
2.4999889731407166
2.518332905239529
encoder pruning ratio =: 0.30
2.5037263896730213
2.9374874101744757
2.945283055305481
2.9452809029155307
2.4999903705385
2.9452831149101257
encoder pruning ratio =: 0.50
2.506667176882426
3.767586456404792
3.7749599350823297
3.774948928091261
2.4999874234199524
3.7749598026275635
encoder pruning ratio =: 0.70
2.5096019506454468
4.966472360822889
4.963240345319112
4.963227457470364
2.4999888208177357
4.96324019961887
encoder pruning ratio =: 0.90
2.5116893781556024
6.966118097305298
6.9532028039296465
6.953175862630208
2.49998496638404
6.953202300601536
decoder pruning ratio = 0.70
encoder pruning ratio =: 0.10
2.496031310823229
2.5145156714651318
2.5140480200449624
2.51404826508628
2.496658682823181
2.514048046535916
encoder pruning ratio =: 0.30
2.5002047883139715
2.9327675236596003
2.9401516914367676
2.940153055720859
2.4966590536965265
2.940151664945814
encoder pruning ratio =: 0.50
2.5029499199655323
3.7686837911605835
3.7760223812527127
3.776021030214098
2.496657815244463
3.7760222752889
encoder pruning ratio =: 0.70
2.5055592788590326
4.978934711880154
4.976641257603963
4.976634913020664
2.496659596761068
4.97664119137658
encoder pruning ratio =: 0.90
2.507223473654853
6.96856231159634
6.957262674967448
6.9572466479407415
2.496657795376248
6.957262542512682
decoder pruning ratio = 0.90
encoder pruning ratio =: 0.10
2.490892549355825
2.510680635770162
2.5104982323116727
2.5104982588026257
2.4911871353785195
2.5104982058207193
encoder pruning ratio =: 0.30
2.4936122761832342
2.912696209218767
2.9178545475006104
2.9178545342551336
2.491187228096856
2.917854573991564
encoder pruning ratio =: 0.50
2.4950863983896046
3.7004877395100064
3.705759366353353
3.7057591809166803
2.491187188360426
3.7057592338985867
encoder pruning ratio =: 0.70
2.4966738290256925
4.819170143869188
4.819966793060303
4.8199668063057794
2.4911871949831643
4.81996684604221
encoder pruning ratio =: 0.90
2.4975650111834207
6.696388191646999
6.69139568010966
6.69139568010966
2.4911872016059027
6.691395706600613
[[2.49544237 2.51452092 2.51396088 2.51395917 2.49620383 2.51396086]
 [2.49988573 2.93344229 2.94131838 2.94131453 2.4962035  2.94131838]
 [2.50305392 3.77316856 3.78100725 3.7809992  2.49620185 3.78100714]
 [2.50600236 4.98125463 4.97726039 4.97724671 2.49620342 4.97726034]
 [2.50825866 6.99740394 6.98298768 6.98296658 2.49619906 6.98298751]]
[[2.4981985  2.51790202 2.5173442  2.51734265 2.4989624  2.51734422]
 [2.50272446 2.93669618 2.9445622  2.94456022 2.49896327 2.94456222]
 [2.50596024 3.77244772 3.78031333 3.78030833 2.498962   3.78031307]
 [2.50901163 4.97455809 4.97071016 4.97070249 2.49896432 4.97070984]
 [2.51129945 6.99228011 6.97806633 6.9780533  2.4989601  6.97806594]]
[[2.49924431 2.51888543 2.51833293 2.51833045 2.49998897 2.51833291]
 [2.50372639 2.93748741 2.94528306 2.9452809  2.49999037 2.94528311]
 [2.50666718 3.76758646 3.77495994 3.77494893 2.49998742 3.7749598 ]
 [2.50960195 4.96647236 4.96324035 4.96322746 2.49998882 4.9632402 ]
 [2.51168938 6.9661181  6.9532028  6.95317586 2.49998497 6.9532023 ]]
[[2.49603131 2.51451567 2.51404802 2.51404827 2.49665868 2.51404805]
 [2.50020479 2.93276752 2.94015169 2.94015306 2.49665905 2.94015166]
 [2.50294992 3.76868379 3.77602238 3.77602103 2.49665782 3.77602228]
 [2.50555928 4.97893471 4.97664126 4.97663491 2.4966596  4.97664119]
 [2.50722347 6.96856231 6.95726267 6.95724665 2.4966578  6.95726254]]
[[2.49089255 2.51068064 2.51049823 2.51049826 2.49118714 2.51049821]
 [2.49361228 2.91269621 2.91785455 2.91785453 2.49118723 2.91785457]
 [2.4950864  3.70048774 3.70575937 3.70575918 2.49118719 3.70575923]
 [2.49667383 4.81917014 4.81996679 4.81996681 2.49118719 4.81996685]
 [2.49756501 6.69638819 6.69139568 6.69139568 2.4911872  6.69139571]]
FINISH
Have a Nice Day
